# -*- coding: utf-8 -*-
"""AllLife Bank Customer Segmentation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sZmTcQm9aXJMa-n8wenR-fiJzKEYBARm

# **Unsupervised Learning Project: AllLife Bank Customer Segmentation**

# **Marks: 30**

Welcome to the project on Unsupervised Learning. We will be using **Credit Card Customer Data** for this project.

--------------------------------
## **Context**
-------------------------------

**AllLife Bank wants to focus on its credit card customer base** in the next financial year. They have been advised by their marketing research team, that the penetration in the market can be improved. Based on this input, the marketing team proposes to run personalized campaigns to target new customers as well as upsell to existing customers.

Another insight from the market research was that the customers perceive the support services of the bank poorly. Based on this, the operations team wants to upgrade the service delivery model, to ensure that customers' queries are resolved faster. The head of marketing and the head of delivery, both decide to reach out to the Data Science team for help.


----------------------------
## **Objective**
-----------------------------

**Identify different segments in the existing customer base**, taking into account their spending patterns as well as past interactions with the bank.

--------------------------
## **About the data**
--------------------------

Data is available on customers of the bank with their credit limit, the total number of credit cards the customer has, and different channels through which the customer has contacted the bank for any queries. These different channels include visiting the bank, online, and through a call center.

- **Sl_no** - Customer Serial Number
- **Customer Key** - Customer identification
- **Avg_Credit_Limit**	- Average credit limit (currency is not specified, you can make an assumption around this)
- **Total_Credit_Cards** - Total number of credit cards
- **Total_visits_bank**	- Total bank visits
- **Total_visits_online** - Total online visits
- **Total_calls_made** - Total calls made

### **Please read the instructions carefully before starting the project.**

This is a commented Jupyter IPython Notebook file in which all the instructions and tasks to be performed are mentioned. Read along carefully to complete the project.

- Blanks '_______' are provided in the notebook that needs to be filled with an appropriate code to get the correct result. Please replace the blank with the right code snippet. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.
- Identify the task to be performed correctly, and only then proceed to write the required code.
- Fill the code wherever asked by the commented lines like "# Fill in the blank" or "# Complete the code". Running incomplete code may throw an error.
- Remove the blank and state your observations in detail wherever the mark down says 'Observations:_____'
- Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.
- You can use the results/observations derived from the analysis here to create your final report.

## **Importing libraries and overview of the dataset**

**Note:** Please make sure you have installed the sklearn_extra library before running the below cell. If you have not installed the library, please run the below code to install the library:

!pip install scikit-learn-extra
"""

!pip install scikit-learn-extra

# Importing all the necessary packages

import pandas as pd

import numpy as np

import matplotlib.pylab as plt

import seaborn as sns

# To scale the data using z-score
from sklearn.preprocessing import StandardScaler

# Importing clustering algorithms
from sklearn.cluster import KMeans

from sklearn.mixture import GaussianMixture

from sklearn_extra.cluster import KMedoids

import warnings
warnings.filterwarnings("ignore")

"""### **Loading the data**"""

data = pd.read_excel('/content/Credit+Card+Customer+Data.xlsx')

data.head()

"""### **Check the info of the data**"""

data.info()

"""**Observations:**

- There are **660 observations and 7 columns** in the dataset.
- All the columns have 660 non-null values, i.e., there are **no missing values**.
- **All the columns are of integer data type**.

**There are no missing values. Let us now figure out the number of unique values in each column.**
"""

data.nunique()

"""- Customer key, which is an identifier, has duplicate values. We will treat the duplicate customer keys before applying any algorithm.

## **Data Preprocessing and Exploratory Data Analysis**

### **Checking duplicate customer keys**

As mentioned above, the Customer Key column has duplicate values. Let's find the rows with duplicate customer keys.

### **Drop the rows with duplicate customer keys**
"""

# There are some duplicates in the column 'Customer Key'. Let us explore

duplicate_keys = data['Customer Key'].duplicated(keep=False)

data[duplicate_keys]

"""- There are **5 duplicate customer keys**. We can **drop these observations**.

**Note:** We are not making any assumptions about which row with the duplicate keys should be dropped. For example, if row 2 and row 10 have duplicate customer keys, then any one of the two rows can be dropped.
"""

# Drop duplicate keys
data = data.drop_duplicates(subset=['Customer Key'], keep='first')

"""### **Dropping columns**

We have done some basic checks. Now, **let's drop the variables that are not required for our analysis**.
"""

data.drop(columns=['Customer Key'], inplace=True)  # Complete the code with actual column names

"""### **Checking duplicate rows**

Now, that we have dropped unnecessary columns, we can again check for duplicates. **Duplicates would mean customers with identical features.**
"""

data[data.duplicated()]

"""- There are 11 duplicate rows. We can drop these duplicate rows from the data."""

data = data[~data.duplicated()]

data.shape

"""- After removing the duplicate keys, the duplicate rows, and dropping unnecessary columns, there are 644 unique observations and 5 columns in our data.

### **Summary Statistics**
"""

data.describe().T

"""**Based on the summary statistics provided, we can observe several key points about the dataset. The dataset contains 655 entries, indicated by the count values for each variable. The Sl_No variable ranges from 1 to 660, suggesting some missing values as the count is less than the maximum value. For the Avg_Credit_Limit, the average credit limit is approximately 34,242.75, with a minimum of 3,000 and a maximum of 200,000, showing a wide range of credit limits among customers. The high standard deviation of 37,240.90 further indicates significant variability in the credit limits.

The Total_Credit_Cards variable shows that customers have an average of 4.69 credit cards, with values ranging from 1 to 10. The standard deviation of 2.17 suggests a moderate spread around the average number of credit cards. Regarding the Total_visits_bank, customers visit the bank on average about 2.40 times, with the number of visits ranging from 0 to 5. The standard deviation of 1.63 indicates some variability in bank visits.

For the Total_visits_online, the average number of online visits is 2.61, with a range from 0 to 15, indicating that some customers heavily use online services. The high standard deviation of 2.94 reflects considerable variability in online visits. Finally, the Total_calls_made variable shows that customers make an average of 3.60 calls, with a range from 0 to 10 calls. The standard deviation of 2.87 suggests variability in the number of calls made by customers. In summary, the dataset exhibits significant variability across different customer behaviors, such as credit limits, the number of credit cards, bank visits, online visits, and calls made. This variability is crucial for understanding and segmenting the customer base effectively.**

**Now, let's go ahead with exploring each variable at hand.**

#### **Let's check the distribution and outliers for each variable in the data.**
"""

# Complete the BELOW code by filling the blanks, before running the cell to avoid any errors

for col in data.columns:
    print(col)

    print('Skew :', round(data[col].skew(), 2))

    plt.figure(figsize=(15, 4))

    plt.subplot(1, 2, 1)

    data[col].hist(bins=30)  # Complete the code

    plt.ylabel('count')

    plt.subplot(1, 2, 2)

    sns.boxplot(x=data[col])  # Complete the code

    plt.show()

"""Based on the visualizations and skewness statistics from the dataset, several observations can be made. Firstly, the Avg_Credit_Limit variable shows a right-skewed distribution with a skewness value of 2.21. This indicates that most customers have a lower average credit limit, with a few having very high limits, as evident from the histogram and the presence of outliers in the boxplot. The Total_Credit_Cards variable appears relatively normally distributed with a slight skewness of 0.16, suggesting most customers have around 3 to 6 credit cards, with fewer customers having either very few or the maximum number of credit cards.

The variables Total_visits_bank and Total_visits_online reveal different customer behaviors. The Total_visits_bank is fairly uniformly distributed, indicating customers visit the bank across all given frequency categories, with a skewness close to zero (0.14). In contrast, the Total_visits_online shows a right-skewed distribution (skewness of 2.23), with most customers making fewer online visits, and some making significantly more, as shown by the outliers. Similarly, Total_calls_made shows a slightly right-skewed distribution (skewness of 0.65), indicating that most customers make fewer calls, with some making a higher number of calls. These insights into the distribution and skewness of customer behavior metrics are crucial for understanding and segmenting the customer base effectively.

### **Checking correlation**
"""

plt.figure(figsize = (8, 8))

sns.heatmap(data.corr(), annot = True, fmt = '0.2f')

plt.show()

"""**Observations:**

- Avg_Credit_Limit is positively correlated with Total_Credit_Cards and Total_visits_online which makes sense.
- Avg_Credit_Limit is negatively correlated with Total_calls_made and Total_visits_bank.
- Total_visits_bank, Total_visits_online, Total_calls_made are negatively correlated which implies that majority of customers use only one of these channels to contact the bank.

### **Scaling the data**
"""

scaler = StandardScaler()

# Standardize the data to have a mean of ~0 and a variance of 1
data_scaled = StandardScaler().fit_transform(data)

"""### **Applying PCA on scaled data**"""

from sklearn.decomposition import PCA

n = data.shape[1]

# Create a PCA instance: pca
pca = PCA(n_components=n)

principal_components = pca.fit_transform(data_scaled)

# Save components to a DataFrame
data_pca = pd.DataFrame(principal_components, columns = data.columns)

# Creating copy of the data to store labels from each algorithm

data_copy = data_pca.copy(deep = True)

"""## **K-Means**

Let us now fit the K-means algorithm on our pca components and find out the optimum number of clusters to use.

We will do this in 3 steps:
1. Initialize a dictionary to store the Sum of Squared Error (SSE) for each K
2. Run for a range of Ks and store SSE for each run
3. Plot the SSE vs K and plot the elbow curve
"""

# step 1
sse = {}

# step 2 - iterate for a range of Ks and fit the pca components to the algorithm.
for k in range(1, 10):
    kmeans = KMeans(n_clusters = k, max_iter = 1000, random_state = 1).fit(data_pca)
    sse[k] = kmeans.inertia_     # Use inertia attribute from the clustering object and store the inertia value for that K

# step 3
plt.figure()

plt.plot(list(sse.keys()), list(sse.values()), 'bx-')

plt.xlabel("Number of cluster")

plt.ylabel("SSE")

plt.show()

# Apply the K-Means algorithm
kmeans = KMeans(n_clusters=3, random_state=42)  # Replace 3 with the desired number of clusters

# Calculate PCA components (assuming 'pca' is your fitted PCA object)
pca_components = pca.transform(data) # Assuming 'data' is your original dataframe

# Fit the kmeans function on the pca components
kmeans.fit(pca_components)

# Adding predicted labels to the original data and the copied data
data_copy = data.copy()

# Save the predictions on the pca components from K-Means
data_copy['Labels'] = kmeans.labels_
data['Labels'] = kmeans.labels_

"""We have generated the labels with K-means. Now, let us look at the various features based on the labels.

### **Creating cluster profiles using the below summary statistics and box plots for each label**
"""

# Number of observations in each cluster
data.Labels.value_counts()

# Calculating summary statistics of the original data for each label
mean = data.groupby('Labels').mean()

median = data.groupby('Labels').median()

df_kmeans = pd.concat([mean, median], axis = 0)

df_kmeans.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']

df_kmeans.T

# Visualizing different features w.r.t K-means labels
data_copy.boxplot(by = 'Labels', layout = (2, 3), figsize = (20, 7)) # Changed layout to accommodate 6 groups

plt.show()

"""**Write Cluster Profiles:_______________**

## **Gaussian Mixture Model**

Let's now create clusters using the Gaussian Mixture Model.
"""

# Apply the Gaussian Mixture algorithm on the pca components with n_components=3 and random_state=1
gmm = GaussianMixture(n_components=3, random_state=1)  # Complete the code

# Fit the model on the pca components
gmm.fit(data_pca)  # Complete the code

# Adding predicted labels to the original data and the copied data
data_copy = data.copy()

data_copy['GmmLabels'] = gmm.predict(data_pca)
data['GmmLabels'] = gmm.predict(data_pca)

# Number of observations in each cluster
data.GmmLabels.value_counts()

# Calculating the summary statistics of the original data for each label
original_features = ["Avg_Credit_Limit", "Total_Credit_Cards", "Total_visits_bank", "Total_visits_online", "Total_calls_made"]

mean = data.groupby('GmmLabels').mean()

median = data.groupby('GmmLabels').median()

df_gmm = pd.concat([mean, median], axis = 0)

df_gmm.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']

df_gmm[original_features].T

# Plotting boxplots with the new GMM based labels

features_with_lables = ["Avg_Credit_Limit", "Total_Credit_Cards", "Total_visits_bank", "Total_visits_online", "Total_calls_made", "GmmLabels"]

data_copy[features_with_lables].boxplot(by = 'GmmLabels', layout = (1, 5),figsize = (20, 7))

plt.show()

"""**Write Cluster Profiles:____________**

**Compare Clusters:____________**

## **K-Medoids**
"""

# Apply the K-Medoids algorithm on the pca components with n_components=3 and random_state=1
kmedo = KMedoids(n_clusters=3, random_state=1)  # Complete the code

# Fit the model on the pca components
kmedo.fit(data_pca)  # Complete the code

# Adding predicted labels to the original data and the copied data
data_copy = data.copy()

data_copy['kmedoLabels'] = kmedo.predict(data_pca)
data['kmedoLabels'] = kmedo.predict(data_pca)

# Number of observations in each cluster
data.kmedoLabels.value_counts()

# Calculating summary statistics of the original data for each label
mean = data.groupby('kmedoLabels').mean()

median = data.groupby('kmedoLabels').median()

df_kmedoids = pd.concat([mean, median], axis = 0)

df_kmedoids.index = ['group_0 Mean', 'group_1 Mean', 'group_2 Mean', 'group_0 Median', 'group_1 Median', 'group_2 Median']

df_kmedoids[original_features].T

# Plotting boxplots with the new K-Medoids based labels

features_with_lables = ["Avg_Credit_Limit", "Total_Credit_Cards", "Total_visits_bank", "Total_visits_online", "Total_calls_made", "kmedoLabels"]

data_copy[features_with_lables].boxplot(by = 'kmedoLabels', layout = (1, 5), figsize = (20, 7))

plt.show()

"""**Write Cluster Profiles:____________**

Let's compare the clusters from K-Means and K-Medoids
"""

comparison = pd.concat([df_kmedoids, df_kmeans], axis = 1)[original_features]

comparison

"""## **Conclusion and Business Recommendations**

Conclusion:
The analysis involved the segmentation of AllLife Bank's credit card customers using various clustering algorithms. Key variables such as average credit limit, total number of credit cards, and customer interactions via bank visits, online visits, and calls were considered. The data showed significant variability in customer behaviors and spending patterns.

We applied several clustering algorithms, including K-Means, Gaussian Mixture Model (GMM), and K-Medoids, on the PCA-transformed components of the data. Each algorithm provided different insights into the customer segments. The results highlighted distinct groups of customers based on their credit limits, usage patterns, and interaction frequency with the bank.
"""